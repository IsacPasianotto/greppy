#+title: Greppy: a trivial grep-like implementation in Python
#+author: Isac Pasianotto

* About

This repo contains the exam code for the course in /"Introduction to tools and methods for research in AI"/, done in A.A. 2024-2025 at the University of Trieste.


* How to run the code

The code is distributed with [[https://pixi.sh/latest/][pixi]] package manager, to set up the environemt you can either spawn a shell with

#+begin_src bash
  pixi shell
#+end_src

or launch the script with

#+begin_src bash
  pixi run python main.py
#+end_src


* Development Branch: The optimization processes:

I have used two tools for the code profiling: ~perf~ and [[https://github.com/plasma-umass/scalene][Scalene]].
*/Disclaimer/* In order to have more meaningful results, before each profilation, run:

#+begin_src bash
  sudo cpupower frequency-set -g performance  #Set the CPU governo  to avoid different frequencies
  sudo sync   # flush filesystem buffers
  sudo sysctl vm.drop_caches=3  # drop caches, 3=all: pagecache, dentries and inodes
#+end_src

** First naive implementation

*** Profiling with Scalene

#+begin_src bash
  pixi run python -m scalene \
      --no-browser \
      main.py -i -f divina_commedia.txt \
      "mezzo del cammin" \
      "Dante" "Beatrice" \
      "Virgilio" \
      "This will not be found"
  mv profile.json profiling/naive.json ; rm profile.html
#+end_src

To see the results:

#+begin_src bash
  pixi run scalene --viewer
#+end_src

and select the file to analize.

*** Profiling with perf

To use per properly we need root privileges, hence:

#+begin_src bash
  sudo pixi shell
  perf stat python main.py \
       -i -f divina_commedia.txt \
      "mezzo del cammin" \
      "Dante" "Beatrice" \
      "Virgilio" \
      "This will not be found" 2> profiling/naive_perf
#+end_src

*** Considerations

Accoringnly to Scalene report, the program took *1.271s* to run. The memory usage is so unsignificant that is lower than the tool's treshold hence is not tracked.

Predictably, the most time consuming function is ~is_pattern_in_line()~ which took the /61%/ of the whole time, followed by ~match_patterns_in_lines()~ with the /25%/ of the time.

The second mentioned function just calls the first one repitetly with all possible combinations of patterns to search and lines, hence the real bottleneck is the first function:

#+begin_src python
  def is_pattern_in_line(line, pattern, ignore_case=False):
    if ignore_case:
        line = line.lower()
        pattern = pattern.lower()

    # Naive pattern matching algorithm
    for i in range(len(line) - len(pattern) + 1):
        match = True
        for j in range(len(pattern)):
            if line[i + j] != pattern[j]:
                match = False
                break
        if match:
            return True
    return False
#+end_src


Surprisingly, the number of chache misses is exttremely low, only the 0.4% even if by design in the code there are a constant number of checks with /if statements/, also for counting the number of matches:

#+begin_src python
  for i, line in enumerate(lines, start=1):
        for pattern in patterns:
            # perform a deep copy to compare later
            snapshot: dict[str, dict[str, int | list[int]]] = {}
            for p in results:
                snapshot[p] = {
                    "occurrences": results[p]["occurrences"].copy(),
                    "counter": results[p]["counter"]
                }

            if is_pattern_in_line(line=line, pattern=pattern, ignore_case=ignore_case):
                results[pattern]["occurrences"].append(i)

            # compare snap with new results
            if snapshot != results:
                results[pattern]["counter"] = len(results[pattern]["occurrences"])
#+end_src


