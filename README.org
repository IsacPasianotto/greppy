#+title: Greppy: a trivial grep-like implementation in Python
#+author: Isac Pasianotto

* About

This repo contains the exam code for the course in /"Introduction to tools and methods for research in AI"/, done in A.A. 2024-2025 at the University of Trieste.


* How to run the code

The code is distributed with [[https://pixi.sh/latest/][pixi]] package manager, to set up the environemt you can either spawn a shell with

#+begin_src bash
  pixi shell
#+end_src

or launch the script with

#+begin_src bash
  pixi run python main.py
#+end_src


* Development Branch: The optimization processes:

I have used two tools for the code profiling: ~perf~ and [[https://github.com/plasma-umass/scalene][Scalene]].
*/Disclaimer/* In order to have more meaningful results, before each profilation, run:

#+begin_src bash
  sudo cpupower frequency-set -g performance  #Set the CPU governo  to avoid different frequencies
  sudo sync   # flush filesystem buffers
  sudo sysctl vm.drop_caches=3  # drop caches, 3=all: pagecache, dentries and inodes
#+end_src

** First naive implementation

*** Profiling with Scalene

#+begin_src bash
  pixi run python -m scalene \
      --no-browser \
      main.py -i -f divina_commedia.txt \
      "mezzo del cammin" \
      "Dante" "Beatrice" \
      "Virgilio" \
      "This will not be found"
  mv profile.json profiling/naive.json ; rm profile.html
#+end_src

To see the results:

#+begin_src bash
  pixi run scalene --viewer
#+end_src

and select the file to analize.

*** Profiling with perf

To use per properly we need root privileges, hence:

#+begin_src bash
  sudo pixi shell
  perf stat python main.py \
       -i -f divina_commedia.txt \
      "mezzo del cammin" \
      "Dante" "Beatrice" \
      "Virgilio" \
      "This will not be found" 2> profiling/naive_perf
#+end_src

*** Considerations

Accoringnly to Scalene report, the program took *1.271s* to run. The memory usage is so unsignificant that is lower than the tool's treshold hence is not tracked.

Predictably, the most time consuming function is ~is_pattern_in_line()~ which took the /61%/ of the whole time, followed by ~match_patterns_in_lines()~ with the /25%/ of the time.

The second mentioned function just calls the first one repitetly with all possible combinations of patterns to search and lines, hence the real bottleneck is the first function:

#+begin_src python
  def is_pattern_in_line(line, pattern, ignore_case=False):
    if ignore_case:
        line = line.lower()
        pattern = pattern.lower()

    # Naive pattern matching algorithm
    for i in range(len(line) - len(pattern) + 1):
        match = True
        for j in range(len(pattern)):
            if line[i + j] != pattern[j]:
                match = False
                break
        if match:
            return True
    return False
#+end_src


Surprisingly, the number of chache misses is exttremely low, only the 0.4% even if by design in the code there are a constant number of checks with /if statements/, also for counting the number of matches:

#+begin_src python
  for i, line in enumerate(lines, start=1):
        for pattern in patterns:
            # perform a deep copy to compare later
            snapshot: dict[str, dict[str, int | list[int]]] = {}
            for p in results:
                snapshot[p] = {
                    "occurrences": results[p]["occurrences"].copy(),
                    "counter": results[p]["counter"]
                }

            if is_pattern_in_line(line=line, pattern=pattern, ignore_case=ignore_case):
                results[pattern]["occurrences"].append(i)

            # compare snap with new results
            if snapshot != results:
                results[pattern]["counter"] = len(results[pattern]["occurrences"])
#+end_src


But this rather than a sign of a good code design, is due to the fact that the code is not very complex and the CPU branch prediction is workging well, probably always guessing a not-matching branch.

** Optimization

*** Remove useless deep copy and comparison

Checking if the dictionary has changed to update the counter is absolutely useless (it was a failure attemp to stress the branch predictor), since the counter will always change if a new match is found.

#+begin_src diff
       for i, line in enumerate(lines, start=1):
         for pattern in patterns:
-            # perform a deep copy to compare later
-            snapshot: dict[str, dict[str, int | list[int]]] = {}
-            for p in results:
-                snapshot[p] = {
-                    "occurrences": results[p]["occurrences"].copy(),
-                    "counter": results[p]["counter"]
-                }
-
             if is_pattern_in_line(line=line, pattern=pattern, ignore_case=ignore_case):
                 results[pattern]["occurrences"].append(i)
-
-            # compare snap with new results
-            if snapshot != results:
                 results[pattern]["counter"] = len(results[pattern]["occurrences"])
-
#+end_src


*** Smarter Pattern Matching Algorithm

As highlighted by the Scalene report, most of the time is spent in the ~is_pattern_in_line()~ method, which is not optimized at all since is a naive double nested loop which checks every single character in the line against the pattern.

A more suitable approach is the [[https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm][Boyer-Moore-Horspool]] algorithm, which will skip some characters in the line if it finds a mismatch, hence reducing the number of checks.

#+begin_src diff
-    # Naive pattern matching algorithm
-    for i in range(len(line) - len(pattern) + 1):
-        match = True
-        for j in range(len(pattern)):
-            if line[i + j] != pattern[j]:
-                match = False
-                break
-        if match:
+    # Boyer-Moore-Horspool
+    line_len: int = len(line)
+    pattern_len: int = len(pattern)
+
+    if pattern_len == 0:
+        return True
+
+    if pattern_len > line_len:
+        return False
+
+    # jump table for bad character
+    skip_table: dict[str, int] = {}
+    for i in range(pattern_len - 1):
+        skip_table[pattern[i]] = pattern_len - 1 - i
+
+    # Actual search
+    i: int = pattern_len - 1
+    while i < line_len:
+        j: int = pattern_len - 1
+        k: int = i
+
+        # Confronta dal fondo
+        while j >= 0 and line[k] == pattern[j]:
+            j -= 1
+            k -= 1
+
+        if j < 0:
             return True
+
+        # Calcola il salto
+        skip: int = skip_table.get(line[k], pattern_len)
+        i += skip
+
     return False
#+end_src




*** Profiling the optimized code

#+begin_src bash
    pixi run python -m scalene \
      --no-browser \
      main.py -i -f divina_commedia.txt \
      "mezzo del cammin" \
      "Dante" "Beatrice" \
      "Virgilio" \
      "This will not be found"
  mv profile.json profiling/optimized.json ; rm profile.html
#+end_src


#+begin_src bash
  sudo pixi shell
  perf stat python main.py\
    -i -f divina_commedia.txt \
    "mezzo del cammin" \
    "Dante" "Beatrice" \
    "Virgilio" \
    "This will not be found" 2> profiling/optimized_perf
#+end_src


** Considerations

The optimized code is way faster: 175ms, wihch is approximately 7.3 times faster. Loocking at the Scalene report, the most time consuming part of the code still the ~is_pattern_in line()~ method (74% of the whole time).
Differently from the previous case where where almost all the time spent in the most relevant part of the code was an /if statement/ (36% of the time), in this case the majority of the time is spent computing the jump table.

The perf report shows an huge increase in the number of branch misses, which raised up to 1.23%. The fact that this is 3 times more than the previous case is due to the fact that the algorithm is more complex and the branch predictor is not able to guess the right branch in some cases, hence it has to flush the pipeline and restart from scratch. Owever, the great improvment in the time saved thanks to the smarter implementation is worth the extra branch misses, which are still very low.

* Second optimization iteration

To increase even further the performance, I also decided to re-implement the Boyer-Moore-Horspool algorithm using [[https://cython.org/][Cython]], which is a superset of Python that allows to write C-like code and compile it to C for better performance.

/Observation/: The comparison between the Cython and the pure python implementation may not be considered fair, in the sense that the profilation of the cython code is done only running the pre-compiled code.
For small projects like this where the overhall execution time is very low, the compilation time is not neglgible, just to have a rought idea, using the ~time~ command, we can see that the compilation time is around 0.9 seconds, which is not negligible for a small project like this, but it is still very low compared to the execution time of the code itself. However, I ignored this time in the profilation, since it is not part of the code execution but rather a one-time setup cost.

#+begin_src bash
    time  pixi run python utils/compile.py build_ext --inplace

    # [ ... ]

  real	0m0.896s
  user	0m0.811s
  sys	0m0.085s
#+end_src


** Profilation results:

Analizing the Scalene report, we can se that the execution time is now 87ms, which menas that compiling the code has doubled the performace of the same code, arriving at a 14.6x speedup compared to the naive implementation.

Almost the whole time is spent in the ~pattern_func~ function, which in this setup is the compiled one. The time spent is 73ms, the 84% of the whole time, which is a great result since that is the core of the algorithm. This means that all the remaining part of the code is almost negligible and, most important, does not introduce any significant slowdown.
