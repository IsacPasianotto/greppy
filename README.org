#+title: Greppy: a trivial grep-like implementation in Python
#+author: Isac Pasianotto

* About

This repo contains the exam code for the course in /"Introduction to tools and methods for research in AI"/, done in A.A. 2024-2025 at the University of Trieste.


* How to run the code

The code is distributed with [[https://pixi.sh/latest/][pixi]] package manager, to set up the environemt you can either spawn a shell with

#+begin_src bash
  pixi shell
#+end_src

or launch the script with

#+begin_src bash
  pixi run python main.py
#+end_src


* Development Branch: The optimization processes:

I have used two tools for the code profiling: ~perf~ and [[https://github.com/plasma-umass/scalene][Scalene]].
*/Disclaimer/* In order to have more meaningful results, before each profilation, run:

#+begin_src bash
  sudo cpupower frequency-set -g performance  #Set the CPU governo  to avoid different frequencies
  sudo sync   # flush filesystem buffers
  sudo sysctl vm.drop_caches=3  # drop caches, 3=all: pagecache, dentries and inodes
#+end_src

** First naive implementation

*** Profiling with Scalene

#+begin_src bash
  pixi run python -m scalene \
      --no-browser \
      main.py -i -f divina_commedia.txt \
      "mezzo del cammin" \
      "Dante" "Beatrice" \
      "Virgilio" \
      "This will not be found"
  mv profile.json profiling/naive.json ; rm profile.html
#+end_src

To see the results:

#+begin_src bash
  pixi run scalene --viewer
#+end_src

and select the file to analize.

*** Profiling with perf

To use per properly we need root privileges, hence:

#+begin_src bash
  sudo pixi shell
  perf stat python main.py \
       -i -f divina_commedia.txt \
      "mezzo del cammin" \
      "Dante" "Beatrice" \
      "Virgilio" \
      "This will not be found" 2> profiling/naive_perf
#+end_src

*** Considerations

Accoringnly to Scalene report, the program took *1.271s* to run. The memory usage is so unsignificant that is lower than the tool's treshold hence is not tracked.

Predictably, the most time consuming function is ~is_pattern_in_line()~ which took the /61%/ of the whole time, followed by ~match_patterns_in_lines()~ with the /25%/ of the time.

The second mentioned function just calls the first one repitetly with all possible combinations of patterns to search and lines, hence the real bottleneck is the first function:

#+begin_src python
  def is_pattern_in_line(line, pattern, ignore_case=False):
    if ignore_case:
        line = line.lower()
        pattern = pattern.lower()

    # Naive pattern matching algorithm
    for i in range(len(line) - len(pattern) + 1):
        match = True
        for j in range(len(pattern)):
            if line[i + j] != pattern[j]:
                match = False
                break
        if match:
            return True
    return False
#+end_src


Surprisingly, the number of chache misses is exttremely low, only the 0.4% even if by design in the code there are a constant number of checks with /if statements/, also for counting the number of matches:

#+begin_src python
  for i, line in enumerate(lines, start=1):
        for pattern in patterns:
            # perform a deep copy to compare later
            snapshot: dict[str, dict[str, int | list[int]]] = {}
            for p in results:
                snapshot[p] = {
                    "occurrences": results[p]["occurrences"].copy(),
                    "counter": results[p]["counter"]
                }

            if is_pattern_in_line(line=line, pattern=pattern, ignore_case=ignore_case):
                results[pattern]["occurrences"].append(i)

            # compare snap with new results
            if snapshot != results:
                results[pattern]["counter"] = len(results[pattern]["occurrences"])
#+end_src


But this rather than a sign of a good code design, is due to the fact that the code is not very complex and the CPU branch prediction is workging well, probably always guessing a not-matching branch.

** Optimization

*** Remove useless deep copy and comparison

Checking if the dictionary has changed to update the counter is absolutely useless (it was a failure attemp to stress the branch predictor), since the counter will always change if a new match is found.

#+begin_src diff
       for i, line in enumerate(lines, start=1):
         for pattern in patterns:
-            # perform a deep copy to compare later
-            snapshot: dict[str, dict[str, int | list[int]]] = {}
-            for p in results:
-                snapshot[p] = {
-                    "occurrences": results[p]["occurrences"].copy(),
-                    "counter": results[p]["counter"]
-                }
-
             if is_pattern_in_line(line=line, pattern=pattern, ignore_case=ignore_case):
                 results[pattern]["occurrences"].append(i)
-
-            # compare snap with new results
-            if snapshot != results:
                 results[pattern]["counter"] = len(results[pattern]["occurrences"])
-
#+end_src


*** Smarter Pattern Matching Algorithm

As highlighted by the Scalene report, most of the time is spent in the ~is_pattern_in_line()~ method, which is not optimized at all since is a naive double nested loop which checks every single character in the line against the pattern.

A more suitable approach is the [[https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore%E2%80%93Horspool_algorithm][Boyer-Moore-Horspool]] algorithm, which will skip some characters in the line if it finds a mismatch, hence reducing the number of checks.

#+begin_src diff
-    # Naive pattern matching algorithm
-    for i in range(len(line) - len(pattern) + 1):
-        match = True
-        for j in range(len(pattern)):
-            if line[i + j] != pattern[j]:
-                match = False
-                break
-        if match:
+    # Boyer-Moore-Horspool
+    line_len: int = len(line)
+    pattern_len: int = len(pattern)
+
+    if pattern_len == 0:
+        return True
+
+    if pattern_len > line_len:
+        return False
+
+    # jump table for bad character
+    skip_table: dict[str, int] = {}
+    for i in range(pattern_len - 1):
+        skip_table[pattern[i]] = pattern_len - 1 - i
+
+    # Actual search
+    i: int = pattern_len - 1
+    while i < line_len:
+        j: int = pattern_len - 1
+        k: int = i
+
+        # Confronta dal fondo
+        while j >= 0 and line[k] == pattern[j]:
+            j -= 1
+            k -= 1
+
+        if j < 0:
             return True
+
+        # Calcola il salto
+        skip: int = skip_table.get(line[k], pattern_len)
+        i += skip
+
     return False
#+end_src




*** Profiling the optimized code

#+begin_src bash
    pixi run python -m scalene \
      --no-browser \
      main.py -i -f divina_commedia.txt \
      "mezzo del cammin" \
      "Dante" "Beatrice" \
      "Virgilio" \
      "This will not be found"
  mv profile.json profiling/optimized.json ; rm profile.html
#+end_src


#+begin_src bash
  sudo pixi shell
  perf stat python main.py\
    -i -f divina_commedia.txt \
    "mezzo del cammin" \
    "Dante" "Beatrice" \
    "Virgilio" \
    "This will not be found" 2> profiling/optimized_perf
#+end_src

** Considerations

The optimized code is way faster: 175ms, wihch is approximately 7.3 times faster. Loocking at the Scalene report, the most time consuming part of the code still the ~is_pattern_in line()~ method (74% of the whole time).
Differently from the previous case where where almost all the time spent in the most relevant part of the code was an /if statement/ (36% of the time), in this case the majority of the time is spent computing the jump table.

The perf report shows an huge increase in the number of branch misses, which raised up to 1.23%. The fact that this is 3 times more than the previous case is due to the fact that the algorithm is more complex and the branch predictor is not able to guess the right branch in some cases, hence it has to flush the pipeline and restart from scratch. Owever, the great improvment in the time saved thanks to the smarter implementation is worth the extra branch misses, which are still very low.
